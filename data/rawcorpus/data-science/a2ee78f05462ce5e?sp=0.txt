Data & Analytics EngineerData & Analytics Engineer - General ElectricAtlanta, GA-•    Over 3 years of work experience in IT industry, which includes experience on big data technologies such as Hadoop ecosystem and spark, developing analytics using python, cloud services. •    Hands on experience on Python, Hadoop, Hive, Hawq, SQL, Spark, Pig, Java, Shell Scripting, AWS Services, JMP, Sqoop, ETL, and Tableau.Willing to relocate: AnywhereAuthorized to work in the US for any employerWork ExperienceData & Analytics EngineerGeneral Electric - Atlanta, GA-October 2016 to Present•    Developed rule based analytics on Time Series sensor data using python to detect anomaly for thermal power generation assets. Implemented analytic to execute in near real time. •    Involved in data driven analytic to predict healthiness of machines using machine learning techniques. •    Performed data extraction, cleaning, transformations using python scripts for different analytic needs. •    Migrated historical data to Hadoop Data Lake. Wrote python and shell scripts. Worked with parquet, JSON file formats. Used Hawq, psql technologies. •    Implemented optimization techniques for data retrieval, storage, and data transfer. •    Reduced data storage by 80% in hdfs, and made data retrieval from Data Lake 3 times faster.  •    Developed and deployed PySpark application to process AWS S3 data. •    Used Tableau for data comparison and analytic health monitoring. Performed data quality check.DeveloperTata Consultancy Services - Hyderabad, Andhra Pradesh-January 2014 to July 2015• The goal of this project was to collect Data from various Sources and Analyze. It had two major parts, Viewer Data Collection System (VDCS) and Viewer Data Distribution System (VDDS). • Used Sqoop to get data from Oracle. Wrote shell scripts. • Worked with JSON, XML data. Converted JSON to Avro file format. • Created Hive tables and did partitions. Wrote UDFs and embedded in Hive. • Wrote Pig scripts. Retrieved data from HBase to validate. • Developed Oozie workflow jobs and scheduled them using Oozie coordinator.DeveloperStateFarm Insurance-June 2012 to December 2013• The part of the project was to create a Data Lake using Hadoop. • Used Sqoop to import data from DB2, Oracle. Loaded files from Mainframe to Hadoop using Informatica. • Developed shell scripts to automate loading of tables from different databases and performed the incremental loads using Sqoop. • Experienced in creating tables and partitions using Hive, and joined multiple tables according to end user requirement.EducationMaster of Science in Data ScienceTexas Tech University - Lubbock, TXAugust 2016Bachelor of Technology in Computer Science and EngineeringJNT UniversityMay 2012SkillsPython (1 year), Hadoop (2 years)Additional Information•    Hadoop Ecosystem         : Hive, Hawq, Pig, Sqoop, Oozie, MapReduce, HDFS, Spark •    Programming languages        : Python, Bash Scripting, Java, R •    Databases        : SQL Server, Oracle, PostgreSql, HBase, DB2 •    AWS Services        : S3, EMR, EC2, IAM •    Tools        : ETL (Informatica, Pentaho), Eclipse, Tableau, JMP, MS office, Git •    Operating Systems                     : Ubuntu, RHEL, UNIX, Windows