
    <!doctype html>
    <html><head><script src="//cdn.optimizely.com/js/7275028.js"></script><meta http-equiv="Content-Type" content="text/html;charset=utf-8" /><title>Data Scientist - Data Scientist - Frisco, TX | Indeed</title><meta name="description" content="Data Scientist's Resume - Data Scientist in Plano, TX. - Find millions of resumes on Indeed.com."/><meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=no" /><meta property="fb:app_id" content="115882278440564" /><meta property="og:type" content="indeedjobs:indeed_resume" /><meta property="og:title" content="Data Scientist - Indeed Resume" /><meta property="og:description" content="Data Scientist's Resume - Data Scientist in Plano, TX. - Find millions of resumes on Indeed.com." /><meta property="og:url" content="https://www.indeed.com/r/a08ef8737dd690c1" /><meta property="og:image" content="http://labkit.indeed.com/static/indeed_fb.png" /><meta property="og:site_name" content="Indeed.com" /><meta name="keywords" content="Data Scientist's Resume,Data Scientist in Plano, TX" /><link rel="canonical" href="https://www.indeed.com/r/a08ef8737dd690c1"/><meta name="resumeId" content="a08ef8737dd690c1" /><meta name="robots" content="noarchive"><link rel="stylesheet" type="text/css" href="/resumes/s/f749a4a/styles-public-compiled.css" media="all" /><script type="text/javascript">window.__JSERROR__=[];window.onerror=function() { window.__JSERROR__.push(arguments)}</script><script type="text/javascript" id="inspectletjs">window.__insp = window.__insp || []; __insp.push(['wid', '1728953977']);
        (function() {
        function ldinsp(){if(typeof window.__inspld != "undefined") return; window.__inspld = 1; var insp = document.createElement('script'); insp.type = 'text/javascript'; insp.async = true; insp.id = "inspsync"; insp.src = ('https:' == document.location.protocol ? 'https' : 'http') + '://cdn.inspectlet.com/inspectlet.js'; var x = document.getElementsByTagName('script')[0]; x.parentNode.insertBefore(insp, x); };
        setTimeout(ldinsp, 500); document.readyState != "complete" ? (window.attachEvent ? window.attachEvent('onload', ldinsp) : window.addEventListener('load', ldinsp, false)) : ldinsp();
        })();
    </script><script type="text/javascript">var googleAnalyticsDomains = ['indeed.com','indeed.com.au','indeed.com.br','indeed.ca','indeed.ch','indeed.cl','indeed.com.co','indeed.de','indeed.es','indeed.fr','indeed.co.uk','indeed.hk','indeed.ie','indeed.co.in','indeed.jp','indeed.com.mx','indeed.nl','indeed.com.sg','indeed.co.za','indeed.ae','indeed.fi','indeed.lu','indeed.com.my','indeed.com.pe','indeed.com.ph','indeed.com.pk','indeed.pt','indeed.co.ve'];
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-90780-1', 'auto', {
              'allowLinker': true
            });
            ga('require', 'linkid');
            ga('require', 'linker');
            ga('linker:autoLink', googleAnalyticsDomains, false, true);
            ga('send', 'pageview');
        </script>
    <script>
        !function(f,b,e,v,n,t,s){if(f.fbq)return;n=f.fbq=function(){n.callMethod?
        n.callMethod.apply(n,arguments):n.queue.push(arguments)};if(!f._fbq)f._fbq=n;
        n.push=n;n.loaded=!0;n.version='2.0';n.queue=[];t=b.createElement(e);t.async=!0;
        t.src=v;s=b.getElementsByTagName(e)[0];s.parentNode.insertBefore(t,s)}(window,
        document,'script','https://connect.facebook.net/en_US/fbevents.js');

        fbq('init', '579216298929618');
        fbq('track', "PageView");
    </script>
    <noscript>
        <img height="1" width="1" style="display:none"
        src="https://www.facebook.com/tr?id=579216298929618&ev=PageView&noscript=1"/>
    </noscript>
<script type="text/javascript">var _sift = _sift || []; _sift.push(['_setAccount', 'fb21e9c129']); _sift.push(['_setUserId', '1bo5ajmslamkje76']); _sift.push(['_trackPageview']);
                (function() {
                    function loadSift() {
                        var sift = document.createElement('script');
                        sift.type = 'text/javascript';
                        sift.async = true;
                        sift.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'dtlilztwypawv.cloudfront.net/s.js';
                        var s = document.getElementsByTagName('script')[0];
                        s.parentNode.insertBefore(sift, s);
                    }
                    if (window.attachEvent) {
                        window.attachEvent('onload', loadSift);
                    } else {
                        window.addEventListener('load', loadSift, false);
                    }
                })();
            </script></head><body  data-tn-originLogType="rexRezGetView" data-tn-olth="df189d746cb1525b9f27398121bd5879"  data-tn-originLogId="1bo5ajmsqamkjf51" data-tn-application="rex"><link rel="stylesheet" type="text/css" href="/resumes/static/css/icons.css" /><script type="text/javascript">
                function dismissLegalNotice() {
                    document.getElementById('legalNoticeContainer').setAttribute("style","display:none");
                    var d = new Date();
                    d.setTime(d.getTime() + (30*24*60*60*1000));
                    var expires = 'expires='+d.toUTCString();
                    document.cookie = 'LNFI=1; ' + expires;
                }
            </script><div id="legalNoticeContainer" style="display:none;"></div><div id="g_nav" data-tn-section="global-nav" ><table width="100%" cellpadding="0" cellspacing="0"><tbody><tr><td nowrap=""><div id="p_nav" data-tn-component="left-nav"><span class="navBi"><a href="https://www.indeed.com?hl=en" title="Find Jobs" id="jobsLink" data-tn-element="jobs-link" data-tn-link>Find Jobs</a></span><span class="navBi"><a href="/resumes" id="rezLink" class="selected" data-tn-element="find-resume-link" data-tn-link>Find Resumes</a></span><span class="navBi"><a href="https://ads.indeed.com?hl=en&amp;cc=US" id="empLink" data-tn-element="employers-link" data-tn-link onclick="if (!this.href.match('&amp;isid=rezsearch&amp;ikw=')) this.href += '&amp;isid=rezsearch&amp;ikw=rezglobalnav';">Employers</a></span></div></td><td align="right" nowrap=""><div id="u_nav" data-tn-component="right-nav">
                    <script type="text/javascript">
                            function isUserOptionsOpen() {
                                    return document.getElementById('userOptions').className == 'open';
                            }

                            function hideUserOptions(label,options) {
                                    options.className = '';
                                    label.className = 'navBi';
                            }

                            function toggleUserOptions(e) {
                                    var options = document.getElementById('userOptions');
                                    var label = document.getElementById('userOptionsLabel');
                                    if ( isUserOptionsOpen() ) {
                                            hideUserOptions( label, options );
                                            if ( !e.keyCode ) {
                                                    label.blur();
                                            }
                                    } else {
                                            options.className = 'open';
                                            label.className = 'navBi active';
                                            document.onclick = function() { hideUserOptions(label,options); document.onclick = function() { }; };
                                            if (e.keyCode && e.keyCode == 13) {
                                                    var fL = gbid('userOptions').getElementsByTagName('a')[0];
                                                    if (fL) { fL.focus(); }
                                            }
                                            else {
                                                    label.blur();
                                            }
                                    }
                                    stopPropagation(e);

                            }

                            function stopPropagation(e) {
                                    var e = e || window.event;
                                    e.stopPropagation ? e.stopPropagation() : e.cancelBubble = true;
                            }
                    </script>
                    <span id="navpromo" class="navBi"><a class="promo_link" href="https://ads.indeed.com/?hl=en&co=US" onclick="if (!this.href.match('&amp;isid=rezsearch&amp;ikw=')) this.href += '&amp;isid=rezsearch&amp;ikw=reznav';">Post a Job</a></span> <span class="navBi"><a id="userOptionsLabel" rel="nofollow" data-tn-element="sign-in" data-tn-link href="/resumes/account/login?dest=%2Fr%2Fa08ef8737dd690c1">Sign in</a></span></td></tr></tbody></table></div><div id="page_frame" ><div id="page_header" ><div id="search_header"><a id="logo_link" href="/resumes"><img id="indeed_logo" src="/resumes/static/images/indeed_resume_header.png" alt="Indeed Resume" /></a><form id="rezsearch" action="/resumes" method="get" autocomplete="off"><table id="search_table"><tr><td><label id="query_label" for="query">what</label></td><td><label id="location_label" for="location">where</label></td><td></td></tr><tr><td><div id="query_input_container" class="input_container"><input autofocus="autofocus" class="query input" id="query" type="text" name="q" value=""/></div></td><td><div id="location_input_container" class="input_container"><input class="location input" id="location" type="text" name="l" /></div></td><td><button type="submit" id="submit" data-tn-element="find-resumes-button" data-tn-link type="submit" class="input_submit"><span class="input-submit-text">Find Resumes</span></button></td></tr><tr><td><div id="query_required_prompt"></div></td><td></td><td id="advanced_container"><a href="/resumes/advanced?sz=" id="advanced_search" class="sl">Advanced Search</a></td></tr></table></form></div></div><div id="page_content" ><div id="page_content"><div id="resume_content"><div data-tn-section="banner-privacy" style="margin-bottom:20px" class="message-banner-info"><table><tr><td width="30" align="left"><img src="/resumes/static/images/info.gif" width="22" height="22" /></td><td><p style="font-weight:bold;font-size:14px">To help ensure jobseeker privacy, some information has been hidden.</p>To see full resume details, log in to your Indeed account or create an account for free.<div style="margin-top:5px"><a id="signInFromBanner" rel="nofollow" data-tn-element="banner-log-in" data-tn-link="redirect" href="/resumes/account/login?dest=%2Fr%2Fa08ef8737dd690c1">Sign in</a>&nbsp; &nbsp;<a id="registerFromBanner" rel="nofollow" data-tn-element="banner-sign-in" data-tn-link="redirect" href="/resumes/account/register?dest=%2Fr%2Fa08ef8737dd690c1">Create an account</a></div></td></tr></table></div><div id="resume" class="hresume" itemscope itemtype='http://schema.org/Person' ><div id="resume_head"></div><div id="resume_body" class="vcard single_form-content"><div id="basic_info_row" class="last basicInfo-content"><div id="basic_info_cell" class="data_display"><h1 id="resume-contact" class="fn " itemprop='name'>Data Scientist</h1><h2 id="headline" itemprop='jobTitle'>Data Scientist</h2><div id="contact_info_container"><div class="adr" itemprop='address' itemscope itemtype='http://schema.org/PostalAddress'><p id="headline_location" class="locality" itemprop='addressLocality'>Frisco, TX</p></div><div class="separator-hyphen">-</div></div><p id="res_summary" class="summary">• Strong ability to analyze data and build/create new business opportunities and identify new customers, strengthen customer relationship with industry&nbsp;<br/>• Over 10+ years of IT experience in Data anlysis, statistical analysis, diagnostics, descriptive, prescriptive, predictive analytics, digital product analysis, machine learning  implementation and maintenance of MS Business intelligence, R, Python and Tableau platforms&nbsp;<br/>• Process and analyse the amount of data using MS Excel 2010, MS Access 2010, MS SQL 2010, R (programming &amp; Statistical), Python programming &amp; analytics, SAS base, Tableau and also in Bigdata technology such as Hadoop/Mapreduce and YARN /HiveSQL, Apache Zoo, Apache Sqoop, Apache Storm, Apache OOzie, Apache Sentry, Kafka, Spark, H2O.ai, ETL with Talend&nbsp;<br/>•  Experience in designing and implementing Big Data - Hadoop environments from scratch and servive oriented architecture solutions&nbsp;<br/>• Experience in UNIX/LINUX Performance monitoring and kernel tuning and Load balancing. Monitor the disk space, CPU utilization and performance of the servers Hands on experience in creating and maintenance of VM  network using oracle Virtualbox&nbsp;<br/>• Demand planning, forecasting, supply chain analysis, sales data exploration and digital product analysis, identified trends in the sales data to lookup additional revenue opportunity,competitor analysis etc. to support Sales and Marketing&nbsp;<br/>• Analyze sets of data for signals, patterns, ways to group data to answer questions and solve complex data puzzles&nbsp;<br/>• Provides advice on the use of data for compiling personnel and statistical reports and preparing personnel action documents.&nbsp;<br/>• Work experience in analytics working with data to convert large volumes of structured and unstructured data into actionable insights and business value.&nbsp;<br/>• Strong expertise in statistical data analysis and numerical methods&nbsp;<br/>• Strong background in manipulating and analyzing complex big data (terabyte/petabyte scale)&nbsp;<br/>• Excellent knowledge of probability distribution and statistical methods&nbsp;<br/>• Dgital product analytical experience using Google Analytics, A/B test and also Bayesian Machine learning approach&nbsp;<br/>• Efficient in: data acquisition, storage, analysis, integration, Predictive modeling, logistic regression, decision trees, data mining methods, forecasting, factor analysis, cluster analysis, ANOVA, neural networks and other advanced statistical and econometric techniques&nbsp;<br/>• Ability to discover hidden insights or complex patterns through advanced data mining technique with R (package:Rattle, Cluster analysis (connectivity, centroid, distribution and density model) Association analysis (arules) , logistic regression analysis (glm), decision tree(rpart,wsrpart),Random Forests, neural networks (nnet), support vector machines(kernlab),  RGoogle Analytics) and Python (Numpy, Scipy, statsmodels, pandas, Blaze, scrapy)&nbsp;<br/>• Skill to train new employees on collecting data, processing data, and analyzing data&nbsp;<br/>• Data Management: Get &amp; Clean, process, and cross-verify the data in R, Python, SAS, SQL, Tableau, MS-Excel&nbsp;<br/>• Expert in Excel Pivot Tables, vlookups, PowerPivot, Advanced functions&nbsp;<br/>• Ability to write code in R, python and T-SQL scripts to manipulate data for data loads and extracts&nbsp;<br/>• Carrying out specified data processing and statistical techniques.&nbsp;<br/>• Ability to graph with software: Line and scatter plots, Bar Charts, Histograms, Pie chart, Dot charts, Box plots, Time series, Error Bars, Multiple Charts types, Multiple Axes, subplots etc.&nbsp;<br/>• Importing and sending out information into HDFS and Hive utilizing Sqoop and Kafka.&nbsp;<br/>• Developed an information pipeline utilizing Kafka and Storm to store data into HDFS&nbsp;<br/>• Solid experience in using Talend, Real-time streaming the information utilizing Spark with Kafka&nbsp;<br/>• Extensive experience in dealing with Relational Database Management Systems, including normalization, stored procedures, constraints, querying, joins, keys, indexes, data import/export, triggers and cursors.&nbsp;<br/>• Designed and integrated various custom reports in several projects using Tableau and Expert in Geo code Mapping/Geographic heat maps&nbsp;<br/>• Ability to explain/present complicated/advanced analytical methodology and results to non-technical audience&nbsp;<br/>• Experience in dynamic report generating/presentation using Rmarkdown, Knitr package in R environment,&nbsp;<br/>• Skilled to generate web development/publishing using Shiny package in R&nbsp;<br/>• Strong expertise in Apache Hadoop/MapReduce on YARN, Apache Sqoop, Apache Storm, Zookeeper, Apache OOzie/Spark/H2O.ai/Kafka/pySpark and AWS to run computing in large valume of data&nbsp;<br/>• Strong knowledge to understand the Product sales/Online Marketing Sales/Healthcare data mining/ analysis/ predictive analysis to deliver high level outcome.&nbsp;<br/>• Extensively worked on using major statistical analysis tools such as R, Python, Spark, H2O.ai, SQL, SAS and Data mining - Theano, Tensorflow Machine Learning&nbsp;<br/>• Experinec in working NoSQL database such as Hbase/Cassandra and aslo open source search engine such as SOLR, Elasticsearch etc.,&nbsp;<br/>• Sufficient knowledge and learning towards certification in AWS in Amazon Firehouse, amazon kinesis streaming and analytics, amazon simple S3, Amazon Redshift, Amazon EMR, Amazon Machine learning, amazon quicksight&nbsp;<br/>• Implemented the workflows using Apache Oozie framework to automate tasks&nbsp;<br/>• Hands-on Experience in virtualization, storage , and Unix/Linux System Engineering&nbsp;<br/>• Constant Blogs reader such as Hyndsight by Rob Hyndman, IBM Big Data Hub Blogs, Kdnuggets for big data and data miing, Kaggle news, Data science news, Data Science Central, forums, videos etc.,&nbsp;<br/>• Excellent interpersonal ability and a very good team leader/Manager with good communication Skills. Out-standing problem solving, ability to work under pressure, decision making skills and fast learning and grasping capability of new emerging technology.&nbsp;<br/>&nbsp;<br/>IT and Programming Skills:&nbsp;<br/>• Ability to work with Data to transform large volume of structured and unstructured data into actionable insights and business value by supervised/unsupervised machine learning technique&nbsp;<br/>• Languages: Proficient in FORTRAN, R, Python MSSQL, HiveSQL Limited experience Perl.&nbsp;<br/>• Statistical software: R 3.1.2, (Rstudio, Rcmdr, Rmarkdown, rpython, Shiny, RGoogle Analytics, rattle, Rapidminer, nlme, gbm, Forecast, AST, ggmap, ggplot2, lattice, reshape, reshape2, data.table, dplyr, RODBC, foreign, sqldf, DBI, RMySQL, xlsx, WriteXLS) Python (Numpy, Pandas, Scipy,  Scikit, statsmodels, Blaze, scrapy,Machine learning-Theano, NLKT and Visulization: Matplotlib,seaborn,scikit-image), SAS […] SAS /BASE, SAS ODS, SSIS(ETL), SSAS, MS-Excel 2010, MS Access 2010&nbsp;<br/>• Databases: My SQL workbench in Snowflake, AWS S3 MS SQL Server, MySQL, T-SQL or Apache HBASE, Apache CASSANDRA,&nbsp;<br/>• Advanced Machine Learning and Data mining algorithms and data mining technique with R (package:Rattle, Cluster analysis, Association analysis (arules) , logistic regression analysis (glm), decision tree(rpart,wsrpart), Random Forests, neural networks (nnet), support vector machines(kernlab), Machine Learning algorithms in Linear Regression, tree based Models, Bayesian Modeling,  Naïve Bayes, KNN, K-means, dimensionality Reduction algorithms, Gradient Boost &amp; Adaboost&nbsp;<br/>• Expert knowledge in Forecast Models:both R and Python Environment: Trend, Forecasting, Timeseries Linear models(Additive and Multiplicative), ARIMA, Means Forest, Structural Time Series Model, Neural NetWork, Theta Method, BATS Model, Exponential Smoothing methods (ETS, SES, HOLT-WINTERS), Random Walk Forest, Double Seaonal Holt-Winters Method, Autoregressive type in ARIMA,&nbsp;<br/>• Advanced Experience in Customer Segment Analysis: RFM, Churn, Cohort analysis, Decile Analysis,Customer life Time Value, Customer Satisfication (NPS), Bayesian approach to Digital products visitor analysis, Association Analysis, Customer Segmentation, Clustering analsys, Decesion Tree, Liner, Generalized liner regression analysis etc.,&nbsp;<br/>• Expert knowledge in MS Office 2010, PowerPoint, MS Visio, MS-Excel, Origin and Access, This would include vlookups, pivot tables, graphs, etc.&nbsp;<br/>• Web-based Visualization tools: Google Analytics,Lumify, Kibna, Tableau, Qlikview , Matplotlib&nbsp;<br/>• Reporting tools: Tableau, Qlikview, Crystal Reports […] SQL server 2015 reporting Service, MS Excel 2016, cognos&nbsp;<br/>• Big Data Technologies: Snowflake, mysql workbench, Linux based Python and R plotform- Hadoop/ Map/Reduce on YARN, Apache sqoop, Apache Storm, Apache Zookeeper, Apache OOzie, Apache sentry/Kafka /Spark/H2O.ai, PySpark/H2O-R/h2o […] and Theano/TensorFlow. And NoSQL database such as Hbase/Cassandra, data modeling with Talend and data vault etc.,&nbsp;<br/>• Strong Knowledge in NLP(rjava, NLP,OpenNLP,Rweka, magrittr, ggmap), Topic Modeling with LDA by topicmodels, Stream analytics using Samplify(TM), Snowflake Cloud computing with R and python&nbsp;<br/>• Sufficient knowledge and learning towards certification in AWS in Amazon Firehouse, amazon kinesis streaming and analytics, amazon simple S3, Amazon Redshift, Amazon EMR, Amazon Machine learning, amazon quicksight and Tensorflow machine learing&nbsp;<br/>• Experience in designing and implementing Big Data - Hadoop environments from scratch and servive oriented architecture solutions, integrate into cloud service (Snowflake, AWS) and deploy on production systems</p><p id="relocation_status">Willing to relocate: Anywhere</p><p id="employment_eligibility">Authorized to work in the US for any employer</p></div></div><div class="section-item workExperience-content"><div><div class="section_title"><h2>Work Experience</h2></div></div><div id="work-experience-items" class="items-container"><div id="workExperience-EeeBe6Zshp6xqoeEKom3Nw" class="work-experience-section "><div class="data_display"><p class="work_title title">Data Scientist</p><div class="work_company" ><span class="bold">NresearchNow</span> <div class="separator-hyphen">-</div> <div class="inline-block" ><span >Plano, TX</span></div></div><div class="separator-hyphen">-</div><p class="work_dates">January 2017 to July 2017</p><p class="work_description">75024&nbsp;<br/>&nbsp;<br/>Nreserachnow is the unique global market research(40+countries) company around the global such as 1, Global Audience and Panel management; 2. Data integration; 3. Smart Automated Market research mobile, Research score, shopper intelligence:&nbsp;<br/>Industry service: 1. Banking, Consumer Packed Good, Healthcare, Hedge Funds, Market Research agency, Media and Advertising, Private equality, Retail and Technology.&nbsp;<br/>&nbsp;<br/>My responsibility is designing and developing a data strategy and analytics platform for detailed market research, Responsible for building scalable distributed data solutions using Hadoop, automation, Fraud Investigation on email invstiagtion Bad domain, fraud servey activity, RedemptionsAirmail redemption, Decile analysis, CEO Profile and panel analytics, Jumper Analysis,  building algorithms to set the new rule to implement fast/accuracy delivery and heavy adhoc analysis, panel analytic, CRM/Royalty Management campaign analysis to transform the new way to improve ROI.&nbsp;<br/>&nbsp;<br/>• Improved/Modified the traditional way analysis and manual updates by building algorithms, set the new rules to capture the fast way of bad domain, survey activity and panel analytics by using density distribution/outlier detection method algorithms&nbsp;<br/>• Usually every analysis will be handled more than Terabyte level volume using bigdata SnowFlake cloud computing platform&nbsp;<br/>• The above process automated by using R/Python/Oozie to run every day without manual inervence&nbsp;<br/>• Fraudulent survey activity investigation such as suspicious enrollment, suspicious profile data, Duplicated Account, speeder (completing survey much faster than expected) Quality Fail (multiple sessions with quality Fail flag), bad domain (example @163.com,@qq.com), Blacklist(Network is bad), Redemptions Abuse (members having multiple accounts and redeeming from same IP address) and Gibberish or abusive language(in survey/communication), PST-Gamer is performing by county wise such as  by high volume/and low volume survey activity and set the rules accordingly.&nbsp;<br/>• In the above fraudulent survey activity, we are performing pattern analysis by seasonal , weekdays, week-end, Gender, Age of people who participating in survey activity&nbsp;<br/>• Built the algorithm and logic for the survey behavior interms of # survey they took overtime, fraudulent account is more on name, email address, IP, login etc.&nbsp;<br/>• Major adhoc analysis is performing by cloud Snowflake, SQLworkbench, MYSQL to deliver client based day to day activity requirements&nbsp;<br/>• Finding the every 30D,90D and 180 servey activity Active_members and  qualified completes, number of survey activity Finish, Number of survey Invite sent, Avg monthly survey panelist 1+completes, new survey panelist 1+ completes with DOI Quarter, year, country&nbsp;<br/>• Calculated emailable, non-emailable, undeliverable, enrollment, response rate, qualification rate, abandon rate, active panel size for 30D, 90D, 180D etc.,&nbsp;<br/>• Analysis is performing on Campaign analysis, Social media analysis, survey module analysis, registration form analysis, recuritmet analysis, cohort analysis, panel analytic, CRM/Royalty Management, capacity analysis&nbsp;<br/>• Used to set new logic and rules for the each industry and automated such as High PQR speeder, quarantine rules for all countries, quarantined reason, Quarantined cost, Fraud_jumper, Fraud Report, homeowners and insurance profile, banking and credit card investigations, each customer poinst earned, points redeemed, travel and hotel airline customers, B2B Mobile changes, mobile updates, resutarants casual visits and e-rewards(interest in dinning, frequency), auto industry global profiler, Research on banking and investing Survey activities.&nbsp;<br/>• Built dashboard #5 and maintain exisiting (&gt;25 dashboard) edit, chanes in SQL code and update/refresh the server as per the partner needs&nbsp;<br/>• Responsible for building scalable distributed data solutions using Hadoop&nbsp;<br/>• Installed and configured Hive, Pig, Sqoop, Flume and Oozie on the Hadoop cluster&nbsp;<br/>•  Setup and benchmarked Hadoop/HBase clusters for internal use&nbsp;<br/>•  Developed Simple to complex Map/reduce Jobs using Hive and Pig&nbsp;<br/>• Optimized Map/Reduce Jobs to use HDFS efficiently by using various compression mechanisms&nbsp;<br/>• Handled importing of data from various data sources, performed transformations using Hive, MapReduce, loaded data into HDFS and Extracted the data from Snowflake cloud into HDFS using Sqoop&nbsp;<br/>•  Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior&nbsp;<br/>• Used UDF&#39;s to implement business logic in Hadoop&nbsp;<br/>• Continuous monitoring and managing the Hadoop cluster using Cloudera Manager&nbsp;<br/>•  Worked with application teams to install operating system, Hadoop updates, patches, version upgrades as required&nbsp;<br/>•  Installed Oozie workflow engine to run multiple Hive and Pig jobs&nbsp;<br/>•  Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Big Data Strategies - Performance Management (non-transactional, social data), Data Exploration, Social Analytics (non-transactional, social data), Decision Science; Designing and developing scalable software architectures; Manipulate massive-scale datasets, Architectures for big data capture, representation, information extraction and fusion.&nbsp;<br/>• In event campaign we used topic model with LDA by topicmodels package to • We are trying different machine learning approach for the survey fraud detection such as (logistic regression model, Boosting approach, SVM and random forest) using different Beta values comapared and ROC curve values to decide the accuracuy of the model, the out performance closely reached by Random forest model and it&#39;s in under testing stage.&nbsp;<br/>• Used Stream analytics using Samplify(TM), online capture tool and implemeneted the fraud survey detection model&nbsp;<br/>• Further plan a head forward us to implement fully automation through machine learning approach in other domain too.&nbsp;<br/>•  Researchnow can now use the Snoflake&#39;s Elastic Data Warehouse to get scalable and performing environment and it&#39;s automation, self management capabilities&nbsp;<br/>• Getting training and towards certification in AWS Cloud based system, which is still  much easier to choose machine learning model and tunning  parameters etc., dynamic deliverable to client needs&nbsp;<br/>&nbsp;<br/>Environment: Snowflake, mysql workbench, Hadoop HDFS, Mapreduce/YARN, HiveQL, Apache Sqoop, Apache ZOO, Apache&nbsp;<br/>OOzie, ETL Talend, H2O.ai, Rstudio, Pyhton, Theano, SQL, Google Analytics, Samplify(TM), MS Excel 2016,&nbsp;<br/>cognos, Tableau,  and  WINDOWS/Linux platform</p></div></div><div id="workExperience-EeeBe6Zsra-xqoeEKom3Nw" class="work-experience-section "><div class="data_display"><p class="work_title title">Predictive Intelligence Analyst</p><div class="work_company" ><span class="bold">Adecco</span> <div class="separator-hyphen">-</div> <div class="inline-block" ><span >Tulsa, OK</span></div></div><div class="separator-hyphen">-</div><p class="work_dates">November 2015 to December 2016</p><p class="work_description">USA&nbsp;<br/>&nbsp;<br/>Dentsply Sirona is an American dental equipment maker and dental consumables producer that markets its products in over 120 countries. It is a leader in consumable dental products. It provides high quality solutions that support the needs of dental professionals around the globe&nbsp;<br/>&nbsp;<br/>Primary responsibility is designing and developing a data strategy and analytics platform for detailed market segmentation, performance assessments, digital product analysis, correlation analysis, Trend analysis, forecasting analsysis to innovate the new way to improve ROI.&nbsp;<br/>• Intialized and implementing the marketing tragedy  through  data research and analysis of the Dentsply&#39;s Financial, customers, product mix and meet their competitive environment&nbsp;<br/>• Imported the data from different database (SQL Server 2012) using SQL Server Management Studio&nbsp;<br/>• Designed and implemented Big Data - Hadoop environments from scratch with multiple cluster nodes and servive oriented architecture solutions&nbsp;<br/>• Planned, installed and configured the distributed Hadoop Clusters&nbsp;<br/>• Ingested data using Sqoop to load data from MySQL to HDFS on regular basis from various sources&nbsp;<br/>•  ConfiguredHadoop tools like Hive, Pig, HBase, Zookeeper, Flume, HBase, Impala and Sqoop&nbsp;<br/>•  Built relational view of data using HCatalog&nbsp;<br/>•  Ingested data into HBase tables from MySQL, Pig and Hive using Sqoop&nbsp;<br/>•  Wrote Batch operation across multiple rows for DDL (Data Definition Language) and DML (Data Manipulation Language) for improvised performance using the client API calls&nbsp;<br/>•  Grouped and filtered data using hive queries, HQL and Pig Latin Scripts&nbsp;<br/>• Implemented partitioning and bucketing in Hive for more efficient querying of data&nbsp;<br/>•  Created workflows in Oozie along with managing/coordinating the jobs and combining multiple jobs sequentially into one unit of work.&nbsp;<br/>• The data has taken to diffenet stages like cleaning, integrating, handling NA&#39;s, removing outliers, building models, testing , and finaly implementing the model to the end users&nbsp;<br/>• Boxplot, Q-Q plot, performed to detect outliers and handled with mean/median values&nbsp;<br/>• Performed descriptive statistic analysis (Boxplot, Q-Qplot, Histogram plot, Distribution graph, mean, median, std, var. etc) to see the hidden Trend/Pattern or Seaonality in the market etc (Using Rstudio, R-Data-Miner)&nbsp;<br/>• Predictive analytics: we built different predictive models such as Random walk Forest, ARIMA, SNAIVE, ETS, Holt-Winters Method, compared accuracy of the models (MAPE),, Goodness-Fit estimated and then end up with Random Walk Forest is the best one( using RStudio, Rforecast, lubridate, plyr packages)&nbsp;<br/>Using the Random Walk Forest model, predicted financial foreast (Dentsply Top Financial and it&#39;s 4 sister Partner Company).Prediction is subdivided as Per day, Monthly, Quarterly, Yearly return. As using this prediction -quarterly, annual budgets are preparaing&nbsp;<br/>• As Dentsply is the global marketers, we further went down to predict, North America, Canadian Market separately, then again  market is divided three Major Area (West, Central, East) and then, Regional wise (Total 18 regions), further predicted by Territory levels (140 Territories)&nbsp;<br/>• Every month beginning, we are running the model, updated the database and then, graphical representation is sent to the BI Manager, Regional Managers, Territory Sales reps.,&nbsp;<br/>• In case, any market goes down than our LCL(Low Confidence Interval limits), then auto alerts are sending to the BI Manager, Respective Regional Manager&nbsp;<br/>• New Product Launch Forecast: Based on the new product sales and short period available data, we used Box-Jenkin Model(ARIMA) Autoregressive Integrated Moving Average, BASS Diffusion model-predicted cumulative sales projection for 20+ new launch products, revered product Sucucces or failure rate in the Market&nbsp;<br/>• EloquaData Analysis/Webanalytics: we targeted to analysis such as 1, Time spend on the website Vs Purchase in the past three days 2,  Time spend on Web site Vs purchase in the following three days, similarly, Note Vs Purcahse, KPI Vs Purchase, we got insights from Customer Purcahse and browsing behavior, website sales by Customers groups (using H2O-R),&nbsp;<br/>• Performed Digital product analysis with Google Analytics, behavior, acquisition, audience conversions, active users trending over the time, and A/B test using Baysesian machine learning approach to predict which products are customer often visiting; based on that low/high basket, we decided to bid the salse by offering incentives, allow installment payment options, bulk order offer etc.,&nbsp;<br/>• We implemented the forecast model for likelihood of Repurchse within 3 days webvisit, we built three model ( GLM= generalized linear regression, DT= Decision Tree, RF= Random Forest Model),  compared the model with&quot;ROC&quot; curve and ended with Random Forest model with multivariate approach best for us.&nbsp;<br/>• We engaged with Engagement Rate/Interaction Rate, Cliks, Click through Rate (CTR), Effective cost per click (eCPC), visits, visitors, Bounce Rate , cost/engaged viists, PageViews/Visits, Cost/Pageview, Conversion Rate, Cost Per Conversion, Revenue Per Visit/page.&nbsp;<br/>• Graphically visualized Histograms, Correlation graph with P values, outliers cheched and replaced, Dentsity plots, mean plot by days, Linear trend plot VS webvisit Hrs, identified new/recent customers&nbsp;<br/>• Customer Segment Analaysis: we focused RFM , Churn, Cohort, Customer Satisfication(NPS), Association analysis, Decesion tree, linear Regression, Glm(generalized linear Regression Model)  analysis in Customer segment, to identify best customers, recent customers, irreqular customers, , high/low spender, and also identified who is likely responding to product offers, Predicted future repurchase rate of customers.&nbsp;<br/>• Data are imported from Database(SQL Server, 2012, MS Access, website url etc.,) to the Rstudio environment&nbsp;<br/>• Our data has the number of attributes such as Account ID, Location ID, Date, Specialty, SalesTierCode, OrderMonths, InvoiceYear, InvoiceMonth, Sales, LocalCurrency.&nbsp;<br/>• Data were cleaned (such as removed duplicated ID, outliers, 0$) before to do the Statistics&nbsp;<br/>• Descriptive statistics were analysed (Min, Max, Mean, Median, Std, Normality test ) and Boxplot, Q-Qplot, Segment distribution graph, Histogram.  Correlation graph estimated using Hierarchical method: Spearson Covariance&nbsp;<br/>• Computed the RFM algorithms to find the Segments(Recency, Frequency, Monetory) , Total RFM score estimated to all Customers in Rstudio&nbsp;<br/>• Predicted future Repurchase rate by: Generalized linear Regression model gives us the best score for classifying the likelihood of repurchase rate of Customers&nbsp;<br/>• The cumulative results are under implementing through Sales/Marketing department to focus the targeting customers with offers, promos, email marketing etc.,&nbsp;<br/>• We built recommendation engine by using item-item collaborative filtering algorithms over the time, the matrix we used dollar value against sales of the product to increase the revenue&nbsp;<br/>• We attempted different machine learning approach in customer segment analysis such as Dimensionality Reduction Algorithms , k-means clustering, SVM and logistic regression, we got close mactch result with k-mean model&nbsp;<br/>• Finally, we implemented the K-means clustering machine learning classification model to run every week and month to find the Segment changing customers and track them who falls in the low segment and target to bring them back to high RFM segment ( To improve ROI).&nbsp;<br/>&nbsp;<br/>Environment: Rstudio, SQL,  Apache Hadoop/Mapreduce/YARN, Zookeeper with JVM, H2O.ai, SparkR-DataMiner, ETL Talend, Theano, NLP, Google Analytics, MS Excel 2013, Python 2.7, HTML5/CSS, UNIX, MS SQL Server 2012, T-SQL, PowerBI  WINDOWS and CentOs-6.5 LINUX platform</p></div></div><div id="workExperience-EeeBe6ZsrbCxqoeEKom3Nw" class="work-experience-section "><div class="data_display"><p class="work_title title">Data Scientist/Predictive Moduler</p><div class="work_company" ><span class="bold">Avalon Tech</span> <div class="separator-hyphen">-</div> <div class="inline-block" ><span >Frisco, TX</span></div></div><div class="separator-hyphen">-</div><p class="work_dates">January 2015 to July 2015</p></div></div><div id="workExperience-EeeBe6ZsrbKxqoeEKom3Nw" class="work-experience-section "><div class="data_display"><p class="work_title title">Volunteering , Texas, USA</p><div class="work_company" ><span class="bold">Texas Tech University</span> <div class="separator-hyphen">-</div> <div class="inline-block" ><span >Tech, Texas, US</span></div></div><div class="separator-hyphen">-</div><p class="work_dates">August 2014 to November 2014</p><p class="work_description">Developed algorithms and numerical methods for the investigating  data mining, Assisted to develop decision tree then tested with prospectively collected data for tuberculosis skin test with Python environment (Numpy, Pandas, Scikit learn, statsmodels)&nbsp;<br/>Trained and managed a team of data analysts of variable skill and practice levels</p></div></div><div id="workExperience-EeeBe6ZsrbGxqoeEKom3Nw" class="work-experience-section "><div class="data_display"><p class="work_title title">Data scientist</p><div class="work_company" ><span class="bold">Avalon Tech</span></div><div class="separator-hyphen">-</div><p class="work_dates">January 2014 to July 2014</p><p class="work_description">is a leading global provider of electronic manufacturing service focused on providing superior qualityend-to-end manufacturing solution to various industry segments, one of the goal is managing an optimized inventory is one of the crucial operations in the electronic  manufacturing industry. Due to increased volatility in market leadership, Avalon provide value added services to the partners in the form of inventory management. Manufacture by forecast is an important to handle effective inventory management.&nbsp;<br/>&nbsp;<br/>• Analysis and Design of application, created UI usingJavascript and HTML5/CSS&nbsp;<br/>•  Developed and tested many features for dashboard using R&nbsp;<br/>• Implemented business logic using R, created backend database T-SQL stored procedures and Jasper Reports&nbsp;<br/>• Exported/Imported data between different data sources using SQL Server Management Studio&nbsp;<br/>• Maintained program libraries, users&#39; manuals and technical documentation&nbsp;<br/>• Managed large datasets using R data frames and MySQL&nbsp;<br/>• Wrote and executed various MYSQL database queries from R using R -MySQL connector and MySQL dB package&nbsp;<br/>• Hands-on experience in writing and reviewing requirements, architecture documents, test plans, design documents, quality analysis and audits&nbsp;<br/>• Demand forecast analysed using various models (Qualitative/Quantitative), causal approach- Econometric models, Input-output models, Simulation models, Life-cycle models  and expanded to time series models and endup with  exponential smoothing models based on accuracy and suggested the partners for the production needs&nbsp;<br/>• Built various graphs for business decision-making using R rattle Package&nbsp;<br/>• Performed troubleshooting, fixed and deployed many R bug fixes of the two main applications that were a main source of data for both customers and internal customer service team&nbsp;<br/>• Involved in running Hadoop streaming jobs to process terabytes of XML format data.&nbsp;<br/>•  Participated in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshops/meetings with various business users.&nbsp;<br/>• Involved in Sqoop, HDFS Put or CopyFromLocal to ingest data&nbsp;<br/>• Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting.&nbsp;<br/>•  Created and maintained technical documentation for launching Hadoop clusters and for executing Hive queries and Pig Scripts. BuildSQL queries for performing various CRUD operations like create, update, read and delete&nbsp;<br/>• Worked with team of executives and managers using on R/Python applications for quantitive RISK management. By measuring Return, Volatality, downside volatility, maxdraw down, sharpe ratio, Sortino Ratio etc.,&nbsp;<br/>Used Pandas library for statistics Analysis, Data munging  with Pandas&nbsp;<br/>• Experience in designing and implementing Big Data - Architecture Hadoop environments with single node and cluster node from scratch and taught to junior analyst/data scientist&nbsp;<br/>• Hands on experience in creating and maintenance of VM  network using oracle Virtualbox&nbsp;<br/>• Assisted the team to perform analysis in  R plotform as well Hadoop/Mapreduce cluster for Big data analytics within R and Hadoop&nbsp;<br/>• Mentored junior product team members in Product Life Cycle Management, how to define strategies for new products, and go-to-market details&nbsp;<br/>&nbsp;<br/>Environment: R, SQL,  Python 2.7, Hadoop/Mapreduce/HiveSQL/Pig/Sqoop HTML5/CSS, UNIX, MS SQL Server 2013, T-SQL, Linux, Shell Script, Tableau and WINDOWS</p></div></div><div id="workExperience-EeeBe6ZsrbOxqoeEKom3Nw" class="work-experience-section "><div class="data_display"><p class="work_title title">Data Analyst/ Research Scientist</p><div class="work_company" ><span class="bold">University of Oklahoma</span></div><div class="separator-hyphen">-</div><p class="work_dates">November 2009 to September 2013</p><p class="work_description">Statistical analysis of Mass spectral data in high throughput screening technology: Summarizing our results, we have shown that multivariate analysis is a powerful tool in analyzing high-throughput mass spectrometer data for Clinical proteomics studies, a combination of PCA and cluster analysis along with appropriate preprocessing of the data, can lead to meaningful and promising results&nbsp;<br/>&nbsp;<br/>Responsibilities:&nbsp;<br/>• The objective of this project is to develop, implement, and evaluate a general and widely applicable framework for detecting&nbsp;<br/>potentially complex and important statistical patterns in large databases&nbsp;<br/>• Complex patterns were analyzed through advanced data mining technique with R (package:Rattle)&nbsp;<br/>• Worked on data mining, modeling and analysis of complexes, high-volume, high-dimensionality data with R&nbsp;<br/>• Computed with linux based Python environment using (Numpy, Scipy, Pands, scikit learn) for validate the models&nbsp;<br/>• Machine learning technique Bayesian neural network used to identify the peptide connectivity by mass spectral data&nbsp;<br/>• Data acquisition, data Management, data storage, upload, plot data, low-level analysis, Baseline correction, peak detection,&nbsp;<br/>processing, data normalization, std.deviation, regression analysis, regression&nbsp;<br/>• Expert use of datamining technique such as classical regression , logistic regression, CART(Classification and Regression&nbsp;<br/>Trees), neural nets, Association rules, sequence analysis etc.,&nbsp;<br/>• Hierarchical cluster analysis was performed on the reduced data matrix using a two-cluster option. Several standard&nbsp;<br/>clustering methods were attempted and we obtained the best separation of the two true groups&nbsp;<br/>• ANOVA, Univariate/multivariate analysis, cutoff selection, quality assessment, detect best combination&nbsp;<br/>biomarker/catalyst: Discuss: Accuracy of data, false discovery rate, and ability to detect low lying spot&nbsp;<br/>• Responsible for creating Prototype, Reports and Dashboards using Shiny package in R environment/Tableau public&nbsp;<br/>• Responsible for direct interaction with the gathering requirements, documenting technical specification.&nbsp;<br/>• Create visualization in Tableau by connecting to various data sources like SQL Server DB, Excel Spreadsheet and&nbsp;<br/>SharePoint etc&nbsp;<br/>• Experiened to handle different level juniors and analysts,  junior scientists&nbsp;<br/>• Experience in UNIX/LINUX Performance monitoring and kernel tuning and Load balancing. Monitor the disk space, CPU&nbsp;<br/>utilization and performance of the servers&nbsp;<br/>• Used GRID dfs To run the jobs&nbsp;<br/>&nbsp;<br/>Environment: R 3.1.2, Python (Numpy,  Scikit, Pandas, Scipy), SAS studio, MS-Excel 2010, 2013, MS Visio, MS Access</p></div></div><div id="workExperience-EeeBe6ZsrbWxqoeEKom3Nw" class="work-experience-section "><div class="data_display"><p class="work_title title">Data Analytic Engineer</p><div class="work_company" ><span class="bold">University of Joseph Fourier</span> <div class="separator-hyphen">-</div> <div class="inline-block" ><span >Grenoble, FR</span></div></div><div class="separator-hyphen">-</div><p class="work_dates">January 2007 to December 2008</p><p class="work_description">• Research in Theoretical chemistry and Magnetic spectroscopy, Data analysis and processing, Development of algorithms and software for Data processing in Biochemical research (Sensor/Biochip development) with  Statistical and machine learning technique&nbsp;<br/>• Worked on data mining , modeling and analysis of complex, high-volume, high-dimesionality data with R, nmrglue-Python&nbsp;<br/>• Used Multivariate Machine Learning tools Scikit and R for data analysis and prediction&nbsp;<br/>• Spectral segmentation and peak picking methods, a Levenberg-Marquardt least squares optimization algorithm which allows fitting parameters to be constrained and a spectral simulation function applicable to an arbitrary number of dimensions, as well as the extensive collections of fast and efficient scientific and computational routines available in NumPy and SciPy&nbsp;<br/>• Convert data stored in a variety of file formats including Bruker, Agilent/Varian, NMRPipe, Sparky, SIMPSON, and Rowland NMR Toolkit, enabling a number of existing programs to be easily interconnected. Using  versatility of nmrglue, representative applications to the visualization of 1D, 2D and 3D NMR spectra, separating interleaved pseudo 3D experiments, covariance processing and analysis of solid-state NMR relaxation data were presented&nbsp;<br/>• GRID Distributed File Systems used to run the jobs, Involved in Installation, Configuration, Integration, Tuning, Backup, Crash recovery, Upgrades, Patching, Monitoring System Performance, System and Network Security and Troubleshooting Linux and Unix Servers. Performed daily performance monitoring on Linux servers for CPU, memory and disk utilization using top, vmstat, mpstat, iotop, htop, free and sar command, and using Nagios. Scheduling jobs and administrative tasks using Cron and Anacron.&nbsp;<br/>• Tranined the research analysts and junior statisticians&nbsp;<br/>Environment: R, SQL Python(nmrglue, Numpy, SciPy), MS Excel 2003/2007, MS Access 2003, Unix/Linux</p></div></div><div id="workExperience-EeeBe6ZsrbaxqoeEKom3Nw" class="work-experience-section "><div class="data_display"><p class="work_title title">BI Data Analyst</p><div class="work_company" ><span class="bold">CheMatech</span> <div class="separator-hyphen">-</div> <div class="inline-block" ><span >FR</span></div></div><div class="separator-hyphen">-</div><p class="work_dates">January 2006 to January 2007</p><p class="work_description">Primary responsibility was designing and developing a data strategy and analytics platform for detailed market segmentation, digital product analysis, performance assessments, correlation analysis, Trend analysis, forecasting analsysis.&nbsp;<br/>Responsibilities:&nbsp;<br/>• Provided support for the marketing function through secondary research and analysis of the company&#39;s customers, product mix and competitive environment&nbsp;<br/>• syndicated and publicly available data sources to gain a better understanding of customers, market segments and trends&nbsp;<br/>• Provided reporting and analytical support to identify trends and to uncover opportunities based on current customer base and market conditions&nbsp;<br/>• Experience in A/B test to compare the websites and product visitors&nbsp;<br/>• Developed marketing dashboards, reports, and visualizations to identify, inform and predict business/market trends and dynamics for Marketing Leadership&nbsp;<br/>• Enhanced processes, and used data-driven insights to improve operational performance&nbsp;<br/>• Builds customer, market, operational and technological knowledge to improve individual and organizational capabilities&nbsp;<br/>• Provided recommendations that influence business decisions related to the pharma drug supply distribution market&nbsp;<br/>• Worked and managed cross-functional projects with finance, sales and sales effectiveness.&nbsp;<br/>• Identified suitable predictive model and executed in R environment with rattle package&nbsp;<br/>• Machine learning Linear ARIMA, hybrid nueral network model applied to time series forecast analysis&nbsp;<br/>• Assisted to implement ad-hoc project and analysis.&nbsp;<br/>• Devised, developed and implemented disaster recovery and archiving procedures&nbsp;<br/>• Created alerts, notifications, and emails for system errors, insufficient resources, fatal database errors, hardware errors, and security breach.&nbsp;<br/>• Create Work flow / Process flow documentation&nbsp;<br/>• Installed custom fonts for reports to use, troubleshooting and performance tuning of reports&nbsp;<br/>• Creating Jobs, scheduling them and setting up email notifications when job fails by configuring database mail&nbsp;<br/>• Installation and Troubleshooting the installation of SQL Server 2005&nbsp;<br/>• Performance tuning of SQL queries by analyzing the execution plans&nbsp;<br/>• Design SSIS packages to download files from FTP, unzip them, decrypt, then load them in to our tables and move the files to archiving folder&nbsp;<br/>• Generated server side T-SQL scripts for data manipulation and validation and created various snapshots and materialized views for remote instance&nbsp;<br/>&nbsp;<br/>Environment: R, Python, SQL, MS SQL, MS-Excel, R2007b, Pentium PC, Windows Vista, UNIX/LINUX, Outlook.</p></div></div><div id="workExperience-EeeBe6ZsrbexqoeEKom3Nw" class="work-experience-section last"><div class="data_display"><p class="work_title title">Research Analyst</p><div class="work_company" ><span class="bold">Covra</span> <div class="separator-hyphen">-</div> <div class="inline-block" ><span >NL</span></div></div><div class="separator-hyphen">-</div><p class="work_dates">August 2003 to August 2005</p><p class="work_description">• Participated in the development of clean environmental/air programs targeting stationary sources of air emissions for the Department of Energy and Environmental Protection by performing quantitative analysis of air quality/Environmental contamination phenomena and assigning costs to air/environment quality impacts.&nbsp;<br/>• Our research involved addressing costs, health impacts and air quality impacts of environmental contaminations, and the use of predictive models to compare program options using suitable R packages&nbsp;<br/>• Developed linear regression machine learning algorithm and data modeling with R&nbsp;<br/>• Web search and data collection, Web data mining, Extract data from website, Data entry and data processing, algorithms (complex data analisis, forecasting , timeseries analysis, Multivariate analysis).&nbsp;<br/>• Used several Python libraries like wxPython, numPY and matplotlib&nbsp;<br/>• Used Pandas API to put the data as time series and tabular format for east timestamp data manipulation and retrieval&nbsp;<br/>• Responsible for the full execution of data collection, conversions, including mapping, extraction, transformation, and validation&nbsp;<br/>• The duties involved interpretation and presentation of air quality model and cost-benefit model results; analysis and management of large data sets (emissions data, cost data, source inventory data); application of statistical and economic analytics; std. deviation, regression analysis, preparation of narrative reports; and preparation of draft regulations etc.&nbsp;<br/>&nbsp;<br/>Environment: SQL, R/S-Plus, Python (Numpy, Pandas, Scipy , matplotlib), MS-Excel 2003, Qlikview, Pentium PC, Windows XP, UNIX/LINUX.</p></div></div></div></div><div class="section-item education-content"><div><div class="section_title"><h2>Education</h2></div></div><div id="education-items" class="items-container"><div id="education-EeeBe6ZsrbmxqoeEKom3Nw" class="education-section "><div class="data_display" itemprop='alumniOf' itemscope itemtype='http://schema.org/EducationalOrganization'><p class="edu_title">PhD in Chemical Scienec</p><div class="edu_school"><span class="bold"itemprop='name'>Pondicherry Central University</span></div></div></div><div id="education-EeeBe6ZsrbuxqoeEKom3Nw" class="education-section "><div class="data_display" itemprop='alumniOf' itemscope itemtype='http://schema.org/EducationalOrganization'><p class="edu_title">M.S.</p><div class="edu_school"><span class="bold"itemprop='name'>Bharathidasan University</span></div></div></div><div id="education-EeeBe6Zsrb2xqoeEKom3Nw" class="education-section last"><div class="data_display" itemprop='alumniOf' itemscope itemtype='http://schema.org/EducationalOrganization'><p class="edu_title">B.S in Bachalor of Science</p><div class="edu_school"><span class="bold"itemprop='name'>Madurai Kamaraj University</span> <div class="separator-hyphen">-</div> <div class="inline-block" itemprop='address' itemscope itemtype='http://schema.org/PostalAddress'><span itemprop='addressLocality'>Madurai, Tamil Nadu</span></div></div></div></div></div></div><div class="section-item skills-content"><div><div class="section_title"><h2>Skills</h2></div></div><div id="skills-items" class="items-container"><div class="data_display"><div class="skill-container resume-element"><span class="skill-text">LINUX (10+ years)</span>, <span class="skill-text">UNIX (10+ years)</span>, <span class="skill-text">R (10+ years)</span>, <span class="skill-text">Big data technology (3 years)</span>, <span class="skill-text">Python (10+ years)</span></div></div></div></div><div class="section-item additionalInfo-content"><div><div class="section_title"><h2>Additional Information</h2></div></div><div id="additionalinfo-items" class="items-container"><div id="additionalinfo-section" class="last"><div class="data_display"><p>Operating Systems: windows […] UNIX/LINUX</p></div></div></div></div></div></div><div id="right_sidebar" data-tn-section="sidebar"><div id="right_sidebar_actions" style="padding-top:0px;"><div id="contact_container" class="sidebar_container"><a id="contact_button" class="button" data-tn-element="resume-contact-signup" data-tn-link="redirect" href="/resumes/account/register?dest=%2Fresumes%2Fadv%2Fsignup%2Fr%2Fa08ef8737dd690c1&amp;from=passport" rel="nofollow">Email Data Scientist</a></div><div id="resume_actions_download" class="resume_actions"><a rel="nofollow" id="download_pdf_button" class="button download_button" data-tn-element="download-resume" data-tn-action-click href="/resumes/account/login?dest=%2Fr%2Fa08ef8737dd690c1/pdf">Download Resume</a></div><div id="resume_actions_save" class="resume_actions"><div id="save_resume_container" class="sidebar_container" style="margin-bottom: 10px"><a rel="nofollow" id="save_resume_button" data-tn-element="save-resume-login" data-tn-link="redirect" class="button sidebar_button anon" href="/resumes/account/register?dest=/r/a08ef8737dd690c1">Save Resume</a></div></div><div id="resume_actions_share" class="resume_actions"><div id="share_by_email_container" class="sidebar_container"><a rel="nofollow" id="share_by_email_action" class="button" data-tn-element="forward-resume-button" data-tn-action-click href="javascript:void0">Forward Resume</a></div></div><div id="resume_actions_contacted" class=""><p>Updated: August 15, 2017</p></div><div id = "recommendation_rightrail"></div><div id="resume_actions_footer"></div></div></div><div class="clear"><!-- --></div></div></div><div class="clear"><!-- --></div><div id="public_footer"><a href="https://www.indeed.com/">©2017 Indeed</a></div></div></div><script type="text/javascript" src="/resumes/s/c5659e9/en_US.js"></script><script type="text/javascript" src="/resumes/s/a6fa565/publicresume-start-compiled.js"></script>
        <script type="text/javascript">
            //<![CDATA[
            (function() {
var searchParams = ""; var site = {"baseIndeedUrl":"https:\u002F\u002Fwww.indeed.com", "inspectletWebsiteId":"1728953977", "baseResumeUrl":"https:\u002F\u002Fwww.indeed.com", "changePasswordUrl":"https:\u002F\u002Fwww.indeed.com\u002Fresumes\u002Faccount\u002Fchangeemail", "baseBillingUrl":"https:\u002F\u002Fbilling.indeed.com", "resumeSearchCrawlEnabled":true, "siftApiKey":"fb21e9c129", "baseMyIndeedUrl":"https:\u002F\u002Fmy.indeed.com", "baseGoUrl":"https:\u002F\u002Fgo.indeed.com", "defaultRadius":25, "baseEmployersUrl":"https:\u002F\u002Femployers.indeed.com", "countryContext":"US", "resumeSearchEnabled":true, "baseAdsUrl":"https:\u002F\u002Fads.indeed.com", "baseCanonicalUrl":"https:\u002F\u002Fwww.indeed.com", "radiusUnit":"mi", "baseSecureUrl":"https:\u002F\u002Fsecure.indeed.com", "googleAnalyticsDomains":["indeed.com","indeed.com.au","indeed.com.br","indeed.ca","indeed.ch","indeed.cl","indeed.com.co","indeed.de","indeed.es","indeed.fr","indeed.co.uk","indeed.hk","indeed.ie","indeed.co.in","indeed.jp","indeed.com.mx","indeed.nl","indeed.com.sg","indeed.co.za","indeed.ae","indeed.fi","indeed.lu","indeed.com.my","indeed.com.pe","indeed.com.ph","indeed.com.pk","indeed.pt","indeed.co.ve"], "statistics":{"numberOfCountries":"60", "monthlyJobSearches":"3", "numberOfLanguages":"28", "monthlyUniqueVisitors":"200"}}; var session = {"ecosystem":{"mindyAvailable":true, "employerServiceAvailable":true, "proctorAvailable":true, "acmeAvailable":true, "userServiceAvailable":true, "autocompleteAvailable":true, "narcissusAvailable":true}, "alternateLanguages":[{"code":"en", "name":"English"},{"code":"es", "name":"español"}], "compiledCSSUrl":"\u002Fresumes\u002Fs\u002Ff749a4a\u002Fstyles-public-compiled.css", "language":"en", "showDownloadPdf":true, "userAgent":{"app":false, "tablet":false, "androidApp":false, "phone":false, "smartPhone":false, "mobile":false, "geoLoc":false, "mobileApp":false}, "useCompiledCSS":true, "locale":"en_US", "useCompiledJavaScript":true, "compiledJavaScriptUrl":"\u002Fresumes\u002Fs\u002Fa6fa565\u002Fpublicresume-start-compiled.js", "showShareByEmail":true, "trackingKey":"", "useVersionizedI18nLocale":true, "versionizedI18nLocaleUrl":"\u002Fresumes\u002Fs\u002Fc5659e9\u002Fen_US.js", "supportsPostalCodes":true}; var request = {"logType":"rexRezGetView", "showPhoneNumber":false, "hashId":"5389286408626634702", "csrfParam":"indeedcsrftoken", "serpWindow":"ResumeSearch-5389286408626634702", "contactUnavailableReason":"NOT_LOGGED_IN", "searchEngineReferral":false, "trackingParams":{"from": "unknown"}, "alertId":"", "ie6":false, "free":false, "key":"68639b8cf6388fe593ce829eb15a85cb616b71907bd64cf684f05ed173238056d16296f0804258a5", "currentUrl":"\u002Fr\u002Fa08ef8737dd690c1", "contactContext":"rezView\u002Frightrail", "turnstileUrl":"\u002F\u002Ft.indeed.com\u002Fs\u002F21bb7b3\u002Fturnstile-compiled.js", "sanitized":false, "serp":"\u002Fresumes", "searchQueryString":"", "isInvalidField":{}, "queryString":"", "logTypeHash":"df189d746cb1525b9f27398121bd5879", "queryExtractedKeywords":[], "csrfToken":"qeFn9jpTp7GsoebrhxxX7Aqvr09dTWB7", "domain":"https:\u002F\u002Fwww.indeed.com", "parameters":{}}; var viewer = {"siftAccountKey":"1bo5ajmslamkje76", "vanityName":"", "resumePublicUrlDisplay":"", "masquerading":false, "savedResume":false, "outOfCountry":false, "groups":{"rexconfirmcontact": 2, "rexbrowseserpstst": 1, "rexhidejobseekernametst": 1, "rexinspectlettst": 1, "rexshareprivacyorforcesignintst": 0, "indcorpaddrtog": 1, "rssquotetupledqueriestst": 1, "rexonepromotst": 3, "rexrefinebydesiredjobtypetst": 1, "rexsearchbysectiontst": 2, "rexfacebookpixeltst": 1, "rezshowskillexptog": 1, "rexsimilarsearches2tst": 1, "rexssltst": 1, "rextimingtog": 1, "jasxacmelinkheadertst": 2, "advw_homepagenoresume": 1, "rextopnavlinkedacctstog": 1, "resumefromaddressindeedmailtoggle": 0, "rexbottog": 1, "rexdownloadpdfredirecttst": 0, "recrezclickinputtst": 1, "rexsearchwaitingfeedbacktst": 1, "rexresumeforwardtog": 0, "rexstyletst": 0, "rexperfaudtst": 1, "rexprimemodaltst": 0, "rexnursingrefinementsmodaltst": 1, "rexpreviewheadertst": 2, "rexonemodaltst": 1, "rexsiftsciencebeacontst": 1}, "hasDradisJob":false, "confirmed":false, "accountKey":"", "privileged":false, "geoLocation":"Greenbelt, MD", "jobseeker":false, "resumePublicUrl":"", "loggedIn":false, "searchEnabledCountries":[{"countryCode":"AR", "displayName":"Argentina"},{"countryCode":"AU", "displayName":"Australia"},{"countryCode":"BE", "displayName":"België"},{"countryCode":"BR", "displayName":"Brasil"},{"countryCode":"CA", "displayName":"Canada"},{"countryCode":"CO", "displayName":"Colombia"},{"countryCode":"ES", "displayName":"España"},{"countryCode":"FR", "displayName":"France"},{"countryCode":"IN", "displayName":"India"},{"countryCode":"IE", "displayName":"Ireland"},{"countryCode":"IT", "displayName":"Italia"},{"countryCode":"MX", "displayName":"México"},{"countryCode":"NL", "displayName":"Nederland"},{"countryCode":"NZ", "displayName":"New Zealand"},{"countryCode":"SG", "displayName":"Singapore"},{"countryCode":"ZA", "displayName":"South Africa"},{"countryCode":"AE", "displayName":"United Arab Emirates"},{"countryCode":"GB", "displayName":"United Kingdom"},{"countryCode":"US", "displayName":"United States"}], "obfuscatedId":"", "defaultLocation":"Greenbelt, MD", "email":""}; var page = {"viewTk": "1bo5ajmsqamkjf51"}; var jobseeker = {"resume": {"anonymous":   false , "basicInformation": {"firstName": "Data Scientist", "lastName": ""}, "militaryBackground":  false }, "resumePublicUrl": "https:\/\/www.indeed.com\/r\/a08ef8737dd690c1", "resumePublicUrlPath": "\/r\/a08ef8737dd690c1", "resumePublicUrlDisplay": "indeed.com\/r\/a08ef8737dd690c1", "accountKey": "a08ef8737dd690c1"};var employer = {"budgetExceeded":false, "featured":false, "hasAdvertiserAccount":false, "introductoryPeriod":false, "canShareResume":false, "locale":{"unicodeLocaleKeys":[], "ISO3Language":"eng", "country":"US", "displayName":"English (United States)", "displayVariant":"", "language":"en", "displayLanguage":"English", "script":"", "unicodeLocaleAttributes":[], "displayCountry":"United States", "ISO3Country":"USA", "variant":"", "extensionKeys":[], "displayScript":""}, "confirmed":false, "bulkContactDisplayPrice":"$0", "advertiserLastContacted":-1, "comped":false, "contactUnavailableReason":"NOT_LOGGED_IN", "advertiserLastContactedEmail":"", "blocked":false, "lastContactedThisJobseekerText":"", "lastContactedThisJobseekerLongText":"", "dradisJobs":[], "lastContactedThisJobseeker":0, "emailContactPrice":{"amount":1.0, "amountMinorLong":100, "scale":2, "minorPart":0, "positive":true, "amountMajor":1.0, "amountMinor":100.0, "amountMinorInt":100, "positiveOrZero":true, "zero":false, "amountMajorLong":1, "negative":false, "amountMajorInt":1, "negativeOrZero":false, "currencyUnit":{"numeric3Code":"840", "symbol":"USD", "code":"USD", "decimalPlaces":2, "currencyCode":"USD", "defaultFractionDigits":2, "pseudoCurrency":false, "numericCode":840}}, "phoneVerificationDisabled":false, "allowedToContact":true, "phoneVerified":false, "introductoryContactsRemaining":0, "advertiserLastContactedText":"", "contactTemplates":[], "needsChargeConfirmation":false, "billingStatus":"NO_BILLING", "bulkContactPrice":{"amount":0.0, "amountMinorLong":0, "scale":2, "minorPart":0, "positive":false, "amountMajor":0.0, "amountMinor":0.0, "amountMinorInt":0, "positiveOrZero":true, "zero":true, "amountMajorLong":0, "negative":false, "amountMajorInt":0, "negativeOrZero":true, "currencyUnit":{"numeric3Code":"840", "symbol":"USD", "code":"USD", "decimalPlaces":2, "currencyCode":"USD", "defaultFractionDigits":2, "pseudoCurrency":false, "numericCode":840}}, "emailContactDisplayPrice":"$1", "shownPrivateContactInfo":false, "primary":true, "requestingRepeatedContact":false, "showMsgTemplateFirst":false, "urlForMsgTemplate":""}; indeed.rex.publicresume.start( page, site, session, request, viewer, jobseeker, employer, "", "1bo5ajmsqamkjf51", searchParams, '68639b8cf6388fe593ce829eb15a85cb616b71907bd64cf684f05ed173238056d16296f0804258a5' );
            })();
</script></body></html>